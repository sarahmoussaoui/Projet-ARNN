{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6kCJ1XWRUnO"
      },
      "source": [
        "# Importing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qABZAu5zSpXD"
      },
      "source": [
        "## Importing the dataset from the UCI repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQpYeu90RROc"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6EbeAUcCP0iI"
      },
      "outputs": [],
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/2/adult.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nAF3VsE4P2aO",
        "outputId": "84b7b964-9566-4bc2-a3d5-da251981a63f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  adult.zip\n",
            "replace Index? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip adult.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e4PUNupTUvC"
      },
      "source": [
        "## Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ip5P32QC-Iz"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm7Wy5dGDlPL"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "211Ppocbz-Rc"
      },
      "outputs": [],
      "source": [
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.26.4\n",
        "!pip install --upgrade --force-reinstall pandas scipy scikit-learn matplotlib seaborn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRyoTFtoTZFI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from catboost import CatBoostClassifier\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XubVoQRbSuDh"
      },
      "source": [
        "## Importing dataset into variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSrLHBbMP5AC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define column names based on the dataset description\n",
        "columns = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
        "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
        "    \"hours-per-week\", \"native-country\", \"income\"\n",
        "]\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv(\"adult.data\", names=columns, sep=\",\\s*\", engine=\"python\") # les colonnes sont s√©par√©es par une virgule (,) suivie d'√©ventuels espaces (\\s* en regex signifie \"z√©ro ou plusieurs espaces\") /  Utilise le moteur de parsing python car l'expression r√©guli√®re (\\s*) n√©cessite un moteur plus flexible que celui par d√©faut (c).\n",
        "test_data = pd.read_csv(\"adult.test\", names=columns, sep=\",\\s*\", engine=\"python\", skiprows=1) # ignore la premiere ligne du fichier ( entete: \"|1x3 Cross-validation\" )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY_pXmgQRfTu"
      },
      "source": [
        "# Exploring the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYHpd8h6RjYl"
      },
      "source": [
        "## Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHKSOe3_Zkfp"
      },
      "outputs": [],
      "source": [
        "# Fusionner les deux datasets pour une meilleure analyse\n",
        "data = pd.concat([train_data, test_data], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDFq3XZcZlA8"
      },
      "outputs": [],
      "source": [
        "# Afficher les types de donn√©es\n",
        "print(\"\\nTypes de donn√©es :\")\n",
        "print(data.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIj2n-JDZtan"
      },
      "outputs": [],
      "source": [
        "# Statistiques des colonnes cat√©gorielles\n",
        "print(\"\\nValeurs uniques par colonne cat√©gorielle :\")\n",
        "for col in data.select_dtypes(include=[\"object\"]).columns:\n",
        "    print(f\"{col}: {data[col].nunique()} valeurs uniques\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyMrkxQ93-cf"
      },
      "source": [
        "De l'affichage on constate:\n",
        "  - age a deux valeurs uniques : on peut appliquer du one-hot encoding\n",
        "  - income a 4 valeurs uniques dont 2 r√©ellement : un **point** existe pour les deux autres ( >50K. et <=50K.) qu'on devra enlever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH5jGMN_10f6"
      },
      "source": [
        "### Explication des statistiques sp√©cifiques\n",
        "\n",
        "- **Moyenne** : `stats.loc[\"mean\"]`\n",
        "- **M√©diane** : `stats.loc[\"50%\"]`\n",
        "- **√âcart-type** : `stats.loc[\"std\"]` (√©lev√© si les valeurs sont tr√®s dispers√©es).\n",
        "- **Valeur minimale** : `stats.loc[\"min\"]`\n",
        "- **Valeur maximale** : `stats.loc[\"max\"]`\n",
        "- **1er quartile (25%)** : `stats.loc[\"25%\"]`\n",
        "- **3e quartile (75%)** : `stats.loc[\"75%\"]`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNDVsOFrcbcb"
      },
      "outputs": [],
      "source": [
        "# Statistiques descriptives des colonnes num√©riques\n",
        "stats = data.describe()\n",
        "\n",
        "# Afficher toutes les statistiques importantes\n",
        "print(\"üìä Statistiques descriptives des variables num√©riques :\\n\")\n",
        "print(stats[1:]) # dont need to show count because its the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE9BkHc24TS9"
      },
      "source": [
        "De l'affichage on constate que:\n",
        "1. **Les variables `capital-gain` et `capital-loss` sont majoritairement nulles**.\n",
        "2. **Le poids final (`fnlwgt`) a une grande dispersion**, une normalisation pourrait √™tre n√©cessaire.  \n",
        "3. **Les niveaux d‚Äô√©ducation sont concentr√©s autour de 9-12 ans**, ce qui peut aider √† segmenter la population.  \n",
        "4. **Les heures travaill√©es sont fortement centr√©es sur 40h**, ce qui est coh√©rent avec des emplois √† temps plein.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bxR3D9GbZ0Zk"
      },
      "outputs": [],
      "source": [
        "print(\"\\nValeurs manquantes par colonne :\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# V√©rifier les valeurs sp√©ciales comme \"?\" qui sont parfois utilis√©es pour indiquer des donn√©es manquantes\n",
        "print(\"\\nValeurs '?' par colonne :\")\n",
        "for col in data.columns:\n",
        "    print(f\"{col}: {sum(data[col] == '?')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ibVNKBzRnVB"
      },
      "source": [
        "## Data vizualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FovM2qVTYvy6"
      },
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pji7fglzRqjZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzVcJoEGY3Zm"
      },
      "outputs": [],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mIrin4sjaQHt"
      },
      "outputs": [],
      "source": [
        "# Tracer des histogrammes pour les variables num√©riques :  visualiser la fr√©quence d'apparition des diff√©rentes valeurs\n",
        "data.hist(figsize=(12, 10), bins=30, edgecolor='black') # diviser les donn√©es en 30 intervalles\n",
        "plt.suptitle(\"Distribution des variables num√©riques\", fontsize=16)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY2-V_FU3jo3"
      },
      "source": [
        "du sh√©ma on remarque que:\n",
        "  - `capital-gain` et `capital-loss` sont fortement concentr√©es en 0 : √ßa peut affecter la classification du mod√®le , ce qui confirme l‚Äôint√©r√™t de les transformer en variables binaires (`has_capital_gain`, `has_capital_loss`).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0b6mUbO-aa-J"
      },
      "outputs": [],
      "source": [
        "categorical_columns = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"race\", \"sex\", \"native-country\"]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "for i, col in enumerate(categorical_columns, 1):\n",
        "    plt.subplot(3, 3, i) # diviser la figure en une grille de 3 lignes et 3 colonnes (6 sous-graphiques au total).\n",
        "    sns.countplot(data=data, y=col, order=data[col].value_counts().index, palette=\"pastel\") # Trie les cat√©gories en fonction de leur fr√©quence (les plus fr√©quentes en haut).\n",
        "    plt.title(f\"R√©partition de {col}\")\n",
        "    plt.xlabel(\"Nombre d'individus\")\n",
        "\n",
        "plt.tight_layout() # eviter chevauchement des graphiques\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX_207YM_NVl"
      },
      "source": [
        "Des graphiques ci-dessus on constate que:\n",
        "  - Le sex male, la race white et la workclass private dominent dans le dataset.\n",
        "  - La population est majoritairement diplom√© d'un Bachelor ou plus\n",
        "  - Pr√©sence de \"?\" indiquant des valeurs manquantes.\n",
        "  - La population est majoritairement am√©ricaine.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slvJKDAaah5J"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data=data, x=\"income\", palette=\"coolwarm\")\n",
        "plt.title(\"R√©partition des classes de revenus\")\n",
        "plt.xlabel(\"Revenu\")\n",
        "plt.ylabel(\"Nombre d'individus\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVtV3ZzEBAJ_"
      },
      "source": [
        "Ce graphique confirme notre remarque: les points devraient etre enlev√©s durant l'etape de nettoyage. (On les enleve maintenant pour visualisation seulement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbegubtLBmax"
      },
      "outputs": [],
      "source": [
        "data[\"income\"] = data[\"income\"].str.replace(\".\", \"\", regex=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GchzKN-japfp"
      },
      "outputs": [],
      "source": [
        "# age influence comment le revenu ?\n",
        "# Create a pivot table for the heatmap\n",
        "age_income_pivot = data.pivot_table(index=\"age\", columns=\"income\", aggfunc=\"size\", fill_value=0)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(age_income_pivot, cmap=\"coolwarm\", annot=False, cbar=True)\n",
        "plt.title(\"R√©partition de l'√¢ge selon le revenu\")\n",
        "plt.xlabel(\"Revenu\")\n",
        "plt.ylabel(\"√Çge\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm2WMQw1EWH_"
      },
      "source": [
        "Du heatmap on remarque:\n",
        "  - parmi les personnes gagnant <=50K, une forte concentration en ceux ag√©s 23ans.\n",
        "  - La majorit√© de la population gagnet <=50k et varient de 18 √† 47ans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9ApLdkEAawt1"
      },
      "outputs": [],
      "source": [
        "# hours per week influence comment le revenu ?\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.boxplot(data=data, x=\"income\", y=\"hours-per-week\", palette=\"coolwarm\")\n",
        "plt.title(\"Heures travaill√©es par semaine en fonction du revenu\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDEpFfCnCUUM"
      },
      "source": [
        "From the boxplot we can visualize:\n",
        "  - lower and upper whiskers : min and max\n",
        "  - lower and upper quartiles Q1 and Q3 : horizantal limites of rectangle\n",
        "  - median : horizontal half of rectangle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1bE6IBMRaBv"
      },
      "source": [
        "# Pre-processing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa92cEGGRvYT"
      },
      "source": [
        "## Cleaning the dataset: missing values, duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fru5eMGtGQUP"
      },
      "source": [
        "### Deleting Spaces in values of type Object (espaces inutiles au d√©but et √† la fin)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnxH-9zWGTZK"
      },
      "outputs": [],
      "source": [
        "# Supprimer les espaces avant/apr√®s les valeurs\n",
        "train_data = train_data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x) # d√©finit une fonction x qui verifie dabord si la colonne est de type object avant de supprimer les espaces\n",
        "test_data = test_data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc9Q5g1vFKpv"
      },
      "source": [
        "### Missing Values in columns: workclass, occupation and native-country"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzWgn-FJFduv"
      },
      "outputs": [],
      "source": [
        "# let's see\n",
        "train_data[['workclass', 'occupation', 'native-country']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbvnKY0eURyp"
      },
      "outputs": [],
      "source": [
        "# Remplacer les \"?\" par NaN car pandas ne reconnait pas ? comme valeur manquante\n",
        "train_data.replace(\"?\", np.nan, inplace=True)\n",
        "test_data.replace(\"?\", np.nan, inplace=True)\n",
        "\n",
        "# V√©rifier les valeurs manquantes\n",
        "print(train_data.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dihmzT7AXTg9"
      },
      "outputs": [],
      "source": [
        "# Remplacer les valeurs manquantes par la valeur la plus fr√©quente (mode)\n",
        "columns = ['workclass', 'occupation', 'native-country']\n",
        "for col in columns:\n",
        "    train_data[col] = train_data[col].fillna(train_data[col].mode()[0]) # mode() retourne l'element le plus frequent en dataframe pandas, [0] our extraire la valeur\n",
        "    test_data[col] = test_data[col].fillna(test_data[col].mode()[0])\n",
        "\n",
        "print(train_data.isnull().sum())  # V√©rifier qu'il n'y a plus de NaN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlOyzqYGOilL"
      },
      "source": [
        "### Fixing the income column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RtK3EtWoOlcM"
      },
      "outputs": [],
      "source": [
        "train_data[\"income\"] = train_data[\"income\"].astype(str).str.replace(\".\", \"\", regex=False)\n",
        "test_data[\"income\"] = test_data[\"income\"].astype(str).str.replace(\".\", \"\", regex=False)\n",
        "\n",
        "# test\n",
        "test_data[\"income\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcSx3eCXL4Yx"
      },
      "source": [
        "### Let's check the duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmPzD8fdKmlS"
      },
      "outputs": [],
      "source": [
        "# verifying it theres duplicates\n",
        "duplicates = train_data.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xOGl1ihGLv8D"
      },
      "outputs": [],
      "source": [
        "# let's see if they are stricly identical\n",
        "duplicate_rows = train_data[train_data.duplicated()]\n",
        "duplicate_rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToMBdF3rMAoR"
      },
      "source": [
        "We conclude that these rows represent different individuals --> not duplicates so we leave them as they are."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CvryiT7R2-T"
      },
      "source": [
        "## Data Transformation: Encoding and scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAmKFWLENBnj"
      },
      "source": [
        "### Binary Encoding\n",
        "Previously during the statistical analysis\n",
        "- **workclass** : 9 valeurs uniques  \n",
        "- **education** : 16 valeurs uniques  \n",
        "- **marital-status** : 7 valeurs uniques  \n",
        "- **occupation** : 15 valeurs uniques  \n",
        "- **relationship** : 6 valeurs uniques  \n",
        "- **race** : 5 valeurs uniques  \n",
        "- **sex** : 2 valeurs uniques  \n",
        "- **native-country** : 42 valeurs uniques  \n",
        "- **income** : 4 valeurs uniques  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KA3Zp8sR7U-"
      },
      "outputs": [],
      "source": [
        "print(train_data[\"income\"].unique())  # Afficher toutes les valeurs uniques de la colonne\n",
        "print(test_data[\"income\"].unique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93cLGouYQQDS"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "train_data[\"income\"] = le.fit_transform(train_data[\"income\"])\n",
        "test_data[\"income\"] = le.transform(test_data[\"income\"])\n",
        "\n",
        "# Check processed data\n",
        "print(train_data[\"income\"].unique())\n",
        "print(test_data[\"income\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiodJS_rQQdO"
      },
      "source": [
        "### One-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LnaVRtxpbYTs"
      },
      "outputs": [],
      "source": [
        "# seeing how many unique values are there in each categorical column --> to apply dummie variables we need to make sure that we create the same number of new binary columns\n",
        "print(len(train_data[\"native-country\"].unique()))\n",
        "print(len(test_data[\"native-country\"].unique()))\n",
        "\n",
        "#confirming\n",
        "missing_cols = set(train_data.columns) - set(test_data.columns)\n",
        "missing_cols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAUaji4XeF9i"
      },
      "source": [
        "result is : 40 and 41 meaning not all values are present"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "auZzeXgWQTLa"
      },
      "outputs": [],
      "source": [
        "train_data = pd.get_dummies(train_data, columns=['workclass', 'marital-status',  # get_dummies : transforme chaque cat√©gorie en une nouvelle colonne binaire (0 ou 1).\n",
        "                                                 'occupation', 'relationship', 'race',\n",
        "                                                 'sex', 'native-country'], drop_first=True) # √âvite la multicolin√©arit√© en supprimant la premi√®re cat√©gorie de chaque variable: r√©duit la redondance dans les donn√©es\n",
        "test_data = pd.get_dummies(test_data, columns=['workclass', 'marital-status',\n",
        "                                               'occupation', 'relationship', 'race',\n",
        "                                               'sex', 'native-country'], drop_first=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgBKQcF8zBDl"
      },
      "source": [
        "we removed education column because education number is sufficient for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wecky8SJzA0g"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.drop(columns=[\"education\"])\n",
        "test_data = test_data.drop(columns=[\"education\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL-y1VFWzLbs"
      },
      "outputs": [],
      "source": [
        "# lets verify\n",
        "print(\"education\" in train_data.columns)  # Doit afficher False\n",
        "print(\"education\" in test_data.columns)   # Doit afficher False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx3QTQfRerYu"
      },
      "outputs": [],
      "source": [
        "missing_cols = set(train_data.columns) - set(test_data.columns)\n",
        "for col in missing_cols:\n",
        "    test_data[col] = 0  # Ajouter les colonnes manquantes avec des valeurs 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1vi4RXfcUDu"
      },
      "outputs": [],
      "source": [
        "# confirming that it's fixed\n",
        "print(len(test_data.columns)==len(train_data.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Fk5p6lUQTw4"
      },
      "source": [
        "### Creation of new variables to avoid missclassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0k3AzjU04qVo"
      },
      "outputs": [],
      "source": [
        "# Cr√©ation des nouvelles variables binaires\n",
        "train_data[\"has_capital_gain\"] = (train_data[\"capital-gain\"] > 0).astype(int)\n",
        "train_data[\"has_capital_loss\"] = (train_data[\"capital-loss\"] > 0).astype(int)\n",
        "\n",
        "test_data[\"has_capital_gain\"] = (test_data[\"capital-gain\"] > 0).astype(int)\n",
        "test_data[\"has_capital_loss\"] = (test_data[\"capital-loss\"] > 0).astype(int)\n",
        "\n",
        "# Suppression des colonnes originales\n",
        "train_data.drop(columns=[\"capital-gain\", \"capital-loss\"], inplace=True)\n",
        "test_data.drop(columns=[\"capital-gain\", \"capital-loss\"], inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QTxHjns8jWq6"
      },
      "outputs": [],
      "source": [
        "# V√©rification que les colonnes ont bien √©t√© supprim√©es\n",
        "print(train_data.head())\n",
        "print(test_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRFR2uOQQbIJ"
      },
      "source": [
        "### Scaling numerical values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-ZkbzDlmflc"
      },
      "source": [
        "On choisit le Standardscaler car on doit bien g√©rer les outliers et les diff√©rences d‚Äô√©chelle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dy2TF_aJXdlh"
      },
      "outputs": [],
      "source": [
        "# S√©lection des colonnes num√©riques\n",
        "num_cols = [\"age\", \"fnlwgt\", \"education-num\", \"hours-per-week\"]\n",
        "\n",
        "# Initialiser le scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_data[num_cols] = scaler.fit_transform(train_data[num_cols])\n",
        "test_data[num_cols] = scaler.transform(test_data[num_cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kDGZE3yEq_e-"
      },
      "outputs": [],
      "source": [
        "# V√©rification que les colonnes ont bien √©t√© normalis√©es\n",
        "print(train_data.head())\n",
        "print(test_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK2IF7Z4rpor"
      },
      "source": [
        "fixing one hot encoding from displating true and false to 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V31B_rZJrpNQ"
      },
      "outputs": [],
      "source": [
        "cat_cols = train_data.select_dtypes(include=['bool']).columns  # S√©lection des colonnes bool√©ennes\n",
        "train_data[cat_cols] = train_data[cat_cols].astype(int)\n",
        "test_data[cat_cols] = test_data[cat_cols].astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sKB4JlDsxgUm"
      },
      "outputs": [],
      "source": [
        "print(train_data.head())\n",
        "print(test_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Gkpz2SzZEa"
      },
      "source": [
        "r√©ordonner les colonnes de la meme maniere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIBnGQ4uzdS4"
      },
      "outputs": [],
      "source": [
        "test_data = test_data[train_data.columns]\n",
        "# V√©rifier si les colonnes sont dans le m√™me ordre\n",
        "print(\"L'ordre est identique :\", list(test_data.columns) == list(train_data.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBqS8SyOSCj4"
      },
      "source": [
        "# Data Splittage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgYvx5OmkCi3"
      },
      "source": [
        "### Separating matrix of features X and target variable y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6brIRZjkIXt"
      },
      "outputs": [],
      "source": [
        "# D√©finir la variable cible\n",
        "target_column = \"income\"\n",
        "\n",
        "# S√©paration des donn√©es en features (X) et labels (y)\n",
        "X_train = train_data.drop(columns=[target_column])\n",
        "y_train = train_data[target_column]\n",
        "\n",
        "X_test = test_data.drop(columns=[target_column])\n",
        "y_test = test_data[target_column]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HKmFmq7PkkWq"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HJacQ1IjknrR"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svoKNQrekWOH"
      },
      "source": [
        "### This is optional:  because we uploaded from the repo with a ready 80% separation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7l8EFqgXkFu"
      },
      "outputs": [],
      "source": [
        "\n",
        "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZHpAqpLSeTq"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyM_fKchSi1n"
      },
      "source": [
        "## Metrics and vizualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIFxz5dh38kK"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, y_prob):\n",
        "    \"\"\"\n",
        "    Evaluate a classification model using Accuracy, Precision, Recall, F1-Score,\n",
        "    Confusion Matrix, and ROC Curve.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true: Actual labels\n",
        "    - y_pred: Predicted labels\n",
        "    - y_prob: Predicted probabilities (for ROC Curve)\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute Metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Print Evaluation Metrics\n",
        "    print(f\"‚úÖ Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"‚úÖ Precision: {precision:.4f}\")\n",
        "    print(f\"‚úÖ Recall: {recall:.4f}\")\n",
        "    print(f\"‚úÖ F1-Score: {f1:.4f}\\n\")\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Compute ROC Curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "    plt.plot([0, 1], [0, 1], color='grey', linestyle='--')  # Diagonal line\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy, precision, recall, f1, roc_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIanYlT_SJlE"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3_tebgJz4fp"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tmFYgPjs4Gp2"
      },
      "outputs": [],
      "source": [
        "# Initialize and train Logistic Regression model\n",
        "model_lr = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
        "model_lr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = model_lr.predict(X_test)\n",
        "y_prob_lr = model_lr.predict_proba(X_test)[:, 1]  # Probabilities for positive class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL0u_6TvsuYy"
      },
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKgFndJfswlr"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "y_prob_knn = knn.predict_proba(X_test)[:, 1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nlbwnxws3Z6"
      },
      "source": [
        "## Naives Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr7zCPXHs6ql"
      },
      "outputs": [],
      "source": [
        "nb = GaussianNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "y_pred_nb = nb.predict(X_test)\n",
        "y_prob_nb = nb.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e7mMJfuz7uW"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gUI_MkKwCv57"
      },
      "outputs": [],
      "source": [
        "# Train SVM Model (with probability=True to get probabilities for ROC curve)\n",
        "svm_model = SVC(kernel='rbf', C=1.0, probability=True, random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "y_prob_svm = svm_model.predict_proba(X_test)[:, 1]  # Probabilities for positive class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MzqF4wzz-lF"
      },
      "source": [
        "## XGboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tozcy60DIjI"
      },
      "outputs": [],
      "source": [
        "# Convert to XGBoost DMatrix format (optional but recommended)\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Define XGBoost parameters\n",
        "params = {\n",
        "    'objective': 'binary:logistic',  # Binary classification\n",
        "    'eval_metric': 'logloss',        # Logarithmic loss\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'n_estimators': 100,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# Train the XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(**params)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_prob_xgb = xgb_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "und7Yg3p0BBR"
      },
      "source": [
        "## Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaAyMHFLCZuT"
      },
      "outputs": [],
      "source": [
        "# Train Decision Tree Model\n",
        "tree_model = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
        "tree_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_tree = tree_model.predict(X_test)\n",
        "y_prob_tree = tree_model.predict_proba(X_test)[:, 1]  # Probabilities for positive class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_kEj7pT0FuF"
      },
      "source": [
        "## Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wz2mN4xCi7H"
      },
      "outputs": [],
      "source": [
        "# Train Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_prob_rf = rf_model.predict_proba(X_test)[:, 1]  # Probabilities for positive class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogRymCKXtHXK"
      },
      "source": [
        "## CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF9rj1y8tJjf"
      },
      "outputs": [],
      "source": [
        "cat = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=0)\n",
        "cat.fit(X_train, y_train)\n",
        "\n",
        "y_pred_cat = cat.predict(X_test)\n",
        "y_prob_cat = cat.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsK2jcM8tc0I"
      },
      "source": [
        "## Light GBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XNsRSE-teid"
      },
      "outputs": [],
      "source": [
        "lgbm = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "lgbm.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lgbm = lgbm.predict(X_test)\n",
        "y_prob_lgbm = lgbm.predict_proba(X_test)[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGJ-1s0L0JGd"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58grPzq9EhJ6"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "E3oiBNziDfjG"
      },
      "outputs": [],
      "source": [
        "# Build Neural Network Model\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
        "    layers.Dense(32, activation='relu'),  # Hidden layer 1\n",
        "    layers.Dense(16, activation='relu'),  # Hidden layer 2\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer (binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "y_prob_nn = model.predict(X_test).flatten()  # Probabilities for the positive class\n",
        "y_pred_nn = (y_prob_nn > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(y_test, y_pred_nn, y_prob_nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3AERecOqPBt"
      },
      "source": [
        "### Viewing loss function history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fRE1m-UEm-_"
      },
      "outputs": [],
      "source": [
        "# Plot Training Loss & Accuracy\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Loss Curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training & Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy Curve\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training & Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr4_-rnaSLU-"
      },
      "source": [
        "# Optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XLuG8HNFA54"
      },
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhclluRAFC9C"
      },
      "outputs": [],
      "source": [
        "# Get feature importance of random forest\n",
        "feature_importance = rf_model.feature_importances_\n",
        "sorted_idx = np.argsort(feature_importance)[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(X_train.shape[1]), feature_importance[sorted_idx], align='center')\n",
        "plt.xticks(range(X_train.shape[1]), X_train.columns[sorted_idx], rotation=90)\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.title(\"Feature Importance in Random Forest\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTJ3IulgSVzi"
      },
      "source": [
        "## Cross-Validation using Stratified K-Fold Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sxqnKcrzfBa"
      },
      "outputs": [],
      "source": [
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGsAvrR2xIC4"
      },
      "outputs": [],
      "source": [
        "cv_scores_lr = cross_val_score(model_lr, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "print(\"Mean Logistic Regression accuracy:\", cv_scores_lr.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsWEY0m1uVdh"
      },
      "outputs": [],
      "source": [
        "# cv_scores_svm = cross_val_score(svm_model, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "# print(\"Mean SVM accuracy:\", cv_scores_svm.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcTs2JGXxX-0"
      },
      "outputs": [],
      "source": [
        "cv_scores_stratified = cross_val_score(rf_model, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "print(\"Mean Stratified K-Fold accuracy for Random Forest:\", cv_scores_stratified.mean())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oOBw2W-x-HT"
      },
      "outputs": [],
      "source": [
        "cv_scores_knn = cross_val_score(knn, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "print(\"KNN - Mean Accuracy:\", np.mean(cv_scores_knn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa9iZXXXyAnb"
      },
      "outputs": [],
      "source": [
        "cv_scores_nb = cross_val_score(nb, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "print(\"Na√Øve Bayes - Mean Accuracy:\", np.mean(cv_scores_nb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6PhMmyjyFr5"
      },
      "outputs": [],
      "source": [
        "cv_scores_xgb = cross_val_score(xgb_model, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "print(\"XGBoost - Mean Accuracy:\", np.mean(cv_scores_xgb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr-StxUlyIgg"
      },
      "outputs": [],
      "source": [
        "cv_scores_dt = cross_val_score(tree_model, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "print(\"Decision Tree - Mean Accuracy:\", np.mean(cv_scores_dt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwI_A_zcyN71"
      },
      "outputs": [],
      "source": [
        "cv_scores_cb = cross_val_score(cat, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "print(\"CatBoost - Mean Accuracy:\", np.mean(cv_scores_cb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZRpjHNkySHV"
      },
      "outputs": [],
      "source": [
        "cv_scores_lgbm = cross_val_score(lgbm, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
        "\n",
        "print(\"LightGBM - Mean Accuracy:\", np.mean(cv_scores_lgbm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBjgrVhoSZUN"
      },
      "source": [
        "## GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDyF_DmdrR5l"
      },
      "outputs": [],
      "source": [
        "param_grid_dt = {\n",
        "    'max_depth': [3, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search_dt = GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=5, scoring='accuracy')\n",
        "grid_search_dt.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters for Decision Tree:\", grid_search_dt.best_params_)\n",
        "\n",
        "# Best model\n",
        "best_dt = grid_search_dt.best_estimator_\n",
        "y_pred_dt = best_dt.predict(X_test)\n",
        "y_prob_dt = best_dt.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "evaluate_model(y_test, y_pred_dt, y_prob_dt, \"Decision Tree\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gbi8C8qraaW"
      },
      "outputs": [],
      "source": [
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='accuracy')\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
        "\n",
        "# Best model\n",
        "best_rf = grid_search_rf.best_estimator_\n",
        "y_pred_rf = best_rf.predict(X_test)\n",
        "y_prob_rf = best_rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "evaluate_model(y_test, y_pred_rf, y_prob_rf, \"Random Forest\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Sc4jdkdt1xz"
      },
      "outputs": [],
      "source": [
        "# D√©finition de la grille de param√®tres\n",
        "param_grid_cat = {\n",
        "    'iterations': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'depth': [4, 6, 8]\n",
        "}\n",
        "\n",
        "# Initialisation du mod√®le\n",
        "cat_model = CatBoostClassifier(verbose=0)  # verbose=0 pour √©viter trop de logs\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search_cat = GridSearchCV(cat_model, param_grid_cat, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_cat.fit(X_train, y_train)\n",
        "\n",
        "# Meilleurs param√®tres\n",
        "print(\"Best Parameters for CatBoost:\", grid_search_cat.best_params_)\n",
        "\n",
        "# Meilleur mod√®le\n",
        "best_cat = grid_search_cat.best_estimator_\n",
        "\n",
        "# Pr√©dictions\n",
        "y_pred_cat = best_cat.predict(X_test)\n",
        "y_prob_cat = best_cat.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# √âvaluation\n",
        "evaluate_model(y_test, y_pred_cat, y_prob_cat, \"CatBoost\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxj692Out385"
      },
      "outputs": [],
      "source": [
        "# D√©finition de la grille de param√®tres\n",
        "param_grid_lgbm = {\n",
        "    'num_leaves': [10, 20, 31],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "# Initialisation du mod√®le\n",
        "lgbm = lgb.LGBMClassifier()\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search_lgbm = GridSearchCV(lgbm, param_grid_lgbm, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_lgbm.fit(X_train, y_train)\n",
        "\n",
        "# Meilleurs param√®tres\n",
        "print(\"Best Parameters for LightGBM:\", grid_search_lgbm.best_params_)\n",
        "\n",
        "# Meilleur mod√®le\n",
        "best_lgbm = grid_search_lgbm.best_estimator_\n",
        "\n",
        "# Pr√©dictions\n",
        "y_pred_lgbm = best_lgbm.predict(X_test)\n",
        "y_prob_lgbm = best_lgbm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# √âvaluation\n",
        "evaluate_model(y_test, y_pred_lgbm, y_prob_lgbm, \"LightGBM\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrwN1nmnt860"
      },
      "outputs": [],
      "source": [
        "# D√©finition de la grille de param√®tres\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9],  # Nombre de voisins\n",
        "    'weights': ['uniform', 'distance'],  # Poids des voisins\n",
        "    'metric': ['euclidean', 'manhattan']  # Type de distance\n",
        "}\n",
        "\n",
        "# Initialisation du mod√®le\n",
        "knn_model = KNeighborsClassifier()\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search_knn = GridSearchCV(knn_model, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_knn.fit(X_train, y_train)  # Normalisation recommand√©e pour KNN\n",
        "\n",
        "# Meilleurs param√®tres\n",
        "print(\"Best Parameters for KNN:\", grid_search_knn.best_params_)\n",
        "\n",
        "# Meilleur mod√®le\n",
        "best_knn = grid_search_knn.best_estimator_\n",
        "\n",
        "# Pr√©dictions\n",
        "y_pred_knn = best_knn.predict(X_test)\n",
        "y_prob_knn = best_knn.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# √âvaluation\n",
        "evaluate_model(y_test, y_pred_knn, y_prob_knn, \"KNN\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKGd8Axat6N2"
      },
      "outputs": [],
      "source": [
        "# D√©finition de la grille de param√®tres\n",
        "param_grid_nb = {\n",
        "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6]  # Hyperparam√®tre pour la r√©gularisation\n",
        "}\n",
        "\n",
        "# Initialisation du mod√®le\n",
        "nb_model = GaussianNB()\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search_nb = GridSearchCV(nb_model, param_grid_nb, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search_nb.fit(X_train, y_train)\n",
        "\n",
        "# Meilleurs param√®tres\n",
        "print(\"Best Parameters for Naive Bayes:\", grid_search_nb.best_params_)\n",
        "\n",
        "# Meilleur mod√®le\n",
        "best_nb = grid_search_nb.best_estimator_\n",
        "\n",
        "# Pr√©dictions\n",
        "y_pred_nb = best_nb.predict(X_test)\n",
        "y_prob_nb = best_nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# √âvaluation\n",
        "evaluate_model(y_test, y_pred_nb, y_prob_nb, \"Naive Bayes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzIPA5XiE4pR"
      },
      "outputs": [],
      "source": [
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['linear', 'rbf', 'poly'],\n",
        "    'gamma': ['scale', 'auto']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(SVC(probability=True), param_grid_svm, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters for SVM:\", grid_search.best_params_)\n",
        "\n",
        "# Train best model\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred_best_svm = best_svm.predict(X_test)\n",
        "y_prob_best_svm = best_svm.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate best model\n",
        "evaluate_model(y_test, y_pred_best_svm, y_prob_best_svm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSmfwGVoEuWY"
      },
      "outputs": [],
      "source": [
        "param_grid_xgb = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 6, 9],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(xgb.XGBClassifier(objective='binary:logistic'), param_grid_xgb, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Train best model\n",
        "best_xgb = grid_search.best_estimator_\n",
        "y_pred_best_xgb = best_xgb.predict(X_test)\n",
        "y_prob_best_xgb = best_xgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate best model\n",
        "evaluate_model(y_test, y_pred_best_xgb, y_prob_best_xgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wcHi6xHS0os"
      },
      "source": [
        "# Results Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qJNlLVLS52s"
      },
      "source": [
        "##Comparing models' performances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuJc07GnpZOP"
      },
      "outputs": [],
      "source": [
        "print(\"Logistic Regression:\\n\")\n",
        "evaluate_model(y_test, y_pred_best_lr, y_prob_best_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InGvxTAkpjWI"
      },
      "outputs": [],
      "source": [
        "# print(\"SVM:\\n\")\n",
        "# evaluate_model(y_test, y_pred_best_svm, y_prob_best_svm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i7g86uJpjKR"
      },
      "outputs": [],
      "source": [
        "print(\"XGboost:\\n\")\n",
        "evaluate_model(y_test, y_pred_best_xgb, y_prob_best_xgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zilUu3R1pi94"
      },
      "outputs": [],
      "source": [
        "print(\"Decision Tree:\\n\")\n",
        "evaluate_model(y_test, y_pred_best_dt, y_prob_best_dt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf_yPq2IpiwJ"
      },
      "outputs": [],
      "source": [
        "print(\"Random Forest:\\n\")\n",
        "evaluate_model(y_test, y_pred_best_rf, y_prob_best_fr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvWyAX6UpiW5"
      },
      "outputs": [],
      "source": [
        "print(\"Neural Network:\\n\")\n",
        "evaluate_model(y_test, y_pred_nn, y_prob_nn)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qABZAu5zSpXD",
        "XubVoQRbSuDh",
        "rY_pXmgQRfTu",
        "pYHpd8h6RjYl",
        "Sa92cEGGRvYT",
        "Fru5eMGtGQUP",
        "WlOyzqYGOilL",
        "lcSx3eCXL4Yx",
        "OAmKFWLENBnj",
        "svoKNQrekWOH"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}